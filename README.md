# ğŸ“Œ AdventureWorks Data Warehouse Project

This project demonstrates the design and implementation of an end-to-end, production-style Data Warehouse using the AdventureWorks OLTP dataset.

The goal is to build a scalable, analytics-ready DWH that centralizes operational data, enables efficient reporting, and reflects real-world data engineering and dimensional modeling practices.

The project focuses on architectural clarity, SQL-based ETL development, and clean analytical modeling, making it suitable as a portfolio-quality implementation.

## ğŸ¯ Project Objectives

The project is considered successful when the following outcomes are achieved:

- **Dimensional Data Model:** A fully defined Star Schema covering Sales, Products, Customers, and Geography domains.  
- **Data Pipeline:** Reliable OLTP â†’ Staging â†’ DWH â†’ Mart data flow.  
- **Data Quality:** Clean, conformed, and analytics-ready datasets.  
- **Performance:** Query-optimized warehouse design supporting reporting and aggregations.  
- **Documentation:** Clear technical documentation including ERDs, star schema diagrams, and data lineage.  
- **Reproducibility:** A fully reproducible setup using version-controlled SQL scripts.  

## ğŸ—ï¸ Architecture Overview

The solution follows a layered warehouse architecture:

- **OLTP Source:** AdventureWorks transactional database.  
- **Staging Layer:** Raw data ingestion with separated extraction and loading logic.  
- **Data Warehouse (DWH):**  
  - Bronze: Minimally transformed raw data.  
  - Silver: Cleaned, standardized, and conformed data.  
  - Gold: Dimensional models (facts & dimensions).  
- **Data Marts & Analytics:** Business-focused analytical views and Power BI datasets.  

> Note: Extraction queries are executed on the OLTP source system, but they are owned and versioned under the staging layer as part of the ETL responsibility.

## ğŸ› ï¸ Tooling & Technologies

- **Database:** Microsoft SQL Server  
- **ETL / ELT:** SQL-based pipelines (Stored Procedures, Views, Batch Jobs)  
- **Modeling:** Kimball Dimensional Modeling (Star Schema)  
- **BI & Analytics:** Power BI  
- **Version Control:** Git & GitHub  
- **Documentation:** Markdown, Mermaid.js, profiling reports  

## âš ï¸ Constraints & Assumptions

**Assumptions:**  
- AdventureWorks OLTP database is available and restored.  
- Batch processing is sufficient (no streaming requirements).  
- Data volumes remain within SQL Server processing limits.  

**Constraints:**  
- Local development environment.  
- Source system structure cannot be modified.  
- Scope intentionally focuses on the Sales domain to maintain clarity and depth.  

## ğŸ“‚ Repository Structure

config/
â”œâ”€â”€ mappings/ # Mapping files between source and DWH
â”œâ”€â”€ environments/ # Environment-specific configs
â””â”€â”€ dq_rules/ # Data quality rules and definitions

docs/
â”œâ”€â”€ architecture/ # ERDs, star schema diagrams
â”œâ”€â”€ modeling/ # Dimensional modeling docs
â””â”€â”€ dq_reports/ # Data profiling and quality reports

src/
â”œâ”€â”€ infrastructure/ # Database & schema setup scripts
â”œâ”€â”€ oltp_exploration/ # Source system analysis & profiling
â”œâ”€â”€ staging/
â”‚ â”œâ”€â”€ extract/ # OLTP source extraction logic
â”‚ â””â”€â”€ load/ # Staging table definitions and load scripts
â”œâ”€â”€ dwh/
â”‚ â”œâ”€â”€ bronze/ # Minimally transformed raw data
â”‚ â”œâ”€â”€ silver/ # Cleaned, standardized, conformed data
â”‚ â””â”€â”€ gold/ # Dimensional models (facts & dimensions)
â”œâ”€â”€ dq/ # Data quality stored procedures / checks
â”œâ”€â”€ pipelines/ # Orchestration placeholders (ADF / Airflow)
â””â”€â”€ test/ # Validation, unit, integration, regression tests

README.md

## ğŸ“˜ Additional Documentation

Detailed step-by-step project planning, design decisions, and implementation notes are maintained in Notion and referenced where applicable.  

**Final Note:** This project is designed to reflect real-world data warehouse development practices, emphasizing clarity, maintainability, and architectural discipline.
